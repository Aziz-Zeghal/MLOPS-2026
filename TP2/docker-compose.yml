services:
  mlflow:
    image: python:3.12-slim
    container_name: mlflow-server
    working_dir: /mlflow
    ports:
      - "8080:8080"
    volumes:
      - ./mlruns:/mlflow/mlruns
    command: bash -c "pip install mlflow && mlflow server --host 0.0.0.0 --port 8080 --backend-store-uri /mlflow/mlruns"

  webservice:
    build: .
    container_name: fastapi-webservice
    ports:
      - "5000:5000"
    environment:
      - MLFLOW_URI=http://mlflow-server:8080
    depends_on:
      - mlflow
    volumes:
      - ./mlruns:/app/mlruns
      - ./mlartifacts:/app/mlartifacts
    command: ["fastapi", "run", "model_api.py", "--port", "5000"]
